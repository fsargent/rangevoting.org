<html>
<head>
<link rel="stylesheet" href="/assets/css/original-layout.css">

<title>
RangeVoting.org - Rating Scale Research
</title>
</head>
<body style="font-family: Arial, sans-serif">

<H1> Rating Scale Research relevant to <a href="RangeVoting.html">score voting</a> </H1>

<p><small>By Warren D. Smith, June 2013. &nbsp;&nbsp; 
(Skip to <a href="#conclusions"> conclusions</a>.)
</small></p>

<p>
We attempt to summarize about 100 years and 100 papers
worth of experimental research on humans trying to use "rating scales," 
focusing on what we can learn about
how "score voting" (also called "range voting") should be conducted.
</p><p>
An older page, which knew comparatively little about this
kind of research and recommended 0-99 score range (and had some good reasons), was
<a href="Why99.html">Why99.html</a>;
that and the present page now will need to be reconciled.
The present page seems to conclude 0-9 is the best scale.
</p>

<h3> Introduction/History </h3>
</p><blockquote>
Of the psychological-sociological measurement methods that depend on human 
judgments, rating scale procedures exceed them all for popularity and use.
<br> &nbsp;&nbsp;&nbsp; 
&ndash; Opening paragraph of Lily &amp; Yoram Neumann: 
<i>Comparison of six lengths of rating scales: student attitudes toward instruction</i>, 
Psychological Reports 48,2 (1981) 399-404.
</blockquote><p>

<p>
J.P.Guilford: <i>Psychometric methods</i>, McGraw (2nd ed) 1954, discusses verbal and
numerical scales in chapter 11. 
Incidentally both numerical and verbal scales could also be done as
"graphical" scales where the voter marks a line segment, or
where the graphical scale is merely used as an aid to comprehension.
</p><p>
This area began with psychologists trying to determine how and how not to design
psychological and sensory tests.   Later, much more research (and research money)
came with the advent of  "market research."  E.g. companies wanted to determine which potential
products would be more or less attractive to consumers, and they wanted to 
design surveys that would work well for answering such questions.  
Plus, surveys themselves are expensive.  Big money was at stake.
</p><p>
And so it happened that by 2013, 
at least 100 scientific papers had been published on this topic
(and perhaps far more! &ndash; this is only the number I personally examined
while preparing this page, albeit of those 100, I carefully read only about 10).
Most of this research was oriented toward rating consumer products of all types, not 
politicians/voting,
although some actually did concern politicians.
</p><p>
Our purpose here is to learn from all that published psychological
research what rating scales should work best for score voting purposes &ndash;
and, in the other direction, to suggest additional research questions.
We'll also present a little new research of our own.
</p>

<h3> Other surveys (not oriented toward "score voting," but useful generally) </h3>

<p>
Hershey H. Friedman &amp; Taiwo Amoo:
<a href="RateRatingScales.html">Rating the rating scales</a>,
J. of Marketing Management 9,3 (Winter 1999) 114-123. 
</p><p>
Jon A. Krosnick &amp; Stanley Presser:
<a href="assets/documents/KrosnickPresserSurvey9.pdf">Question and Questionnaire Design</a>,
chapter 9 of <i>Handbook of Survey Research</i>,
Emerald Group Publishing (2nd ed.) 2010.
</p><p>
John P. Robinson, Phillip R. Shaver, Lawrence S. Wrightsman:
Measures of political attitudes,
Academic Press 1999. (801 pages.)
<!-- JA 74.5.M44 1999 Reviewed:
Edward G. Carmines: The Public Opinion Quarterly 65,1 (Spring 2001) 141-143 
-->
</p>

<h3> Example &ndash; The "9-point hedonic scale" </h3>
<p>
was developed for the US Army in the 1940s and 1950s. 
It became standard in the food industry, and
by the year 1995 had become the most widely used sensory scale in the world.
</p>
<table bgcolor="PapayaWhip"><tr>
<td colspan="3">
Overall, how much do you like or dislike this soup?
</td></tr>
<tr><td>
<FORM>
<P><INPUT TYPE="radio" name="hedsc" value="9"> Like extremely
<br><INPUT TYPE="radio" name="hedsc" value="8"> Like very much
<br><INPUT TYPE="radio" name="hedsc" value="7"> Like moderately
<br><INPUT TYPE="radio" name="hedsc" value="6"> Like slightly
<br><nobr><INPUT TYPE="radio" name="hedsc" value="5"> Neither like nor dislike</nobr>
<br><INPUT TYPE="radio" name="hedsc" value="4"> Dislike slightly
<br><INPUT TYPE="radio" name="hedsc" value="3" checked="checked"> Dislike moderately
<br><INPUT TYPE="radio" name="hedsc" value="2"> Dislike very much
<br><INPUT TYPE="radio" name="hedsc" value="1"> Dislike extremely
</P></FORM>
</td><td>&nbsp;</td><td>
<TABLE border="1" align="center" cellspacing="0">
<tr>
<td width="11%" align="center">Like extremely</td>
<td width="11%" align="center">Like very much</td>
<td width="11%" align="center">Like moderately</td>
<td width="11%" align="center">Like slightly</td>
<td width="11%" align="center">Neither like nor dislike</td>
<td width="11%" align="center">Dislike slightly</td>
<td width="11%" align="center">Dislike moderately</td>
<td width="11%" align="center">Dislike very much</td>
<td width="11%" align="center">Dislike extremely</td>
</tr>
</TABLE>
</td><tr></table>
<p>
This is both a verbal and 1-9 numerical scale.  (The numerical value of
"dislike extremely" equals 1, and "like extremely"=9, but the tasters are only
told the words; the numbers are used only by those processing taster questionnaires.)
When used, the scale is either equispaced vertically as shown at left, 
or equispaced horizontally as at right (and always oriented in the directions shown).
</p>
<p>
Although nobody claims this scale is optimal, a lot of research went into developing it.
N-level scales were tried for <nobr>N&isin;{5, 6, 7, 8, 9, 11}.</nobr>
At least 51 words and phrases were investigated as level-descriptors.
Both "balanced" (i.e. symmetrical) and "unbalanced" (ultimately not used) scales were investigated.
The final choice of an equispaced 9-level balanced scale with graphical aid
was chosen based on measured <i>time</i> required for 
tasters to complete questionnaires, <i>reliability</i>/reproducibility of ratings obtained,
transmitted useful discrimination <i>information</i>, and
the fact that 9 levels fit better on standard size paper &ndash;
9 and 11 levels worked best
judged by the other criteria, but 11 caused typesetting problems.  To quote Peryam 1989 (p.23):
</p><blockquote>
Why does the hedonic scale have nine categories, rather than more or less? Economy perhaps? 
Preliminary investigation had shown that discrimination between foods and reliability 
tended to increase up to eleven categories, but we encountered, in addition to the dearth 
of appropriate adverbs, a mechanical problem due to equipment limitations. 
Official government paper was only 8 inches wide and we found that typing eleven 
categories horizontally was not possible. So we sacrificed a theoretical
modicum of precision...
</blockquote><p>
In addition to at least 5 papers written by the developers of this scale, e.g.
</p><blockquote>
L.V.Jones, D.R.Peryam, L.L.Thurstone: Development of a scale for measuring soldier's 
food preferences, Food Research 20 (1955) 512-520.
<br>
D.R.Peryam &amp;  N.F.Girardot:
Advanced taste-test method, Food Engineering 24 (1952) 58-61.
<br>
D.R.Peryam &amp; F.J.Pilgrim:
Hedonic scale method of measuring food preferences, Food Technology (Sep.1957) 9-14.
<br>
David R.Peryam: Reflections, pp.21-30 in 1989 book <i>Sensory evaluation</i> 
(ASTM International, Conshohocken PA).
</blockquote>
<p>
at least one entire symposium was held about it, and scientific
papers continue to be written about it up to the present day. For example
</p><blockquote>
Laura Nicolas, Coline Marquilly, Michael O'Mahony:
The 9-point hedonic scale: Are words and numbers compatible?,
Food Quality and Preference 21,8 (Dec. 2010) 1008-1015
</blockquote>
<p>
found that just using the underlying <i>digits</i> 9,8,7,6,5,4,3,2,1 as descriptors
yields statistically different  results than the official phrases.
A discrepancy was also found using {+4,+3,+2,+1,0,-1,-2,-3,-4}.
</p><p>
That leads to the question of which is better &ndash; <b>numbers or words?</b>
At least on one non-food application (financial risk assessment by auditors)
it was found that numbers were (by certain statistical quality measures) superior:
</p><blockquote>
D.N.Stone &amp; W.N.Dilla:
When numbers are bettter than words &ndash; the 
joint effects of response representation and experience of inherent risk judgments,
Auditing, a journal of practice and theory 13 (1994) 1-19 supplement S.
</blockquote>


<h3> Directionality </h3>

<p>
Some researchers have detected small (of the order of 5%) directional-bias effects, e.g:
</p><blockquote>
Jason Chan:
Response order effects in Likert-type scales,
Educational and Psychological Measurement 51 (1991) 531-540.
<!-- http://www3.nccu.edu.tw/~jyjan/papers/responseorder.pdf -->
<br>
<!-- http://www.amstat.org/sections/srms/proceedings/papers/1993_133.pdf -->
Hershey H. Friedman, Paul J. Herskovitz, Simcha Pollack:
<a href="assets/documents/FrDirBias.pdf">The biasing effects 
of scale-checking styles on response to a Likert scale</a>,
Proc. of the American Statistical Association Annual Conference: Survey Research Methods (1994)
792-795.
<br>
Michaela W&auml;nke, Norbert Schwarz, Elisabeth Noelle-Neumann:
Asking Comparative Questions: The Impact of the Direction of Comparison,
Public Opinion Quarterly 59,3 (1995) 347-372.
<!-- http://poq.oxfordjournals.org/content/59/3/347.abstract -->
<br>
Michaela W&auml;nke:
Comparative Judgments as a Function of the Direction of Comparison Versus Word Order,
Public Opinion Quarterly 60,3 (1996) 400-409.
<!-- http://poq.oxfordjournals.org/content/60/3/400.abstract -->
</blockquote><p>
These effects are not always present.  Anyhow, the lesson of this for
voting purposes is:
<i>all scales must be oriented in a <u>single</u> mandated direction.</i>
</p><blockquote>
Leah Melani Christian,
Nicholas L. Parsons,
Don A. Dillman:
Designing Scalar Questions for Web Surveys,
Sociological Methods &amp; Research 37,3 (Feb. 2009) 393-425
</blockquote><p>
found that orienting scales positive end first 
<i>increased response times.</i>
The lesson of that is: the orientation used in the (horizontal version of) the
9-point hedonic scale &ndash; <i>most-liked at right</i> &ndash; was the correct choice.
</p>

<h3> How many scale-levels? </h3>

<!--
Cliff Holmes:
A statistical evaluation of rating scales,
Journal of the Market Research Society 16,2 (April 1974) 87-107.
[Holmes died in January 2013:  http://www.mrweb.com/drno/news16680.htm ]

Krosnick+Presser:
The American National Election Study surveys have measured citizens'
political attitudes over the last 60 years using 2-, 3-, 4-, 5-, 7-, and 101-point scales 
(W.E.Miller, 1982). Robinson, Shaver, and Wrightsman's (1999) catalog of rating scales 
for a range of social psychological constructs and political attitudes describes 
37 using 2-point scales, 
7 using 3-point scales, 
10 using 4-point scales, 
27 using 5-point scales, 
6 using 6-point scales, 
21 using 7-point scales, 
zero using 8-point scales, 
two using 9-point scales, 
and 
one using a 10-point scale. 
Rating scales used to measure public approval of the U.S. president's job 
performance vary from 2 to 5 points (Morin, 1993; Sussman, 1978). 
Thus, there appears to be no standard for the number of points on rating scales, 
and common practice varies widely.
-->
<p>
About 70 studies of this topic prior to 1980 were reviewed by 
</p><blockquote>
Eli P. Cox III: The optimal number of response alternatives for a scale &ndash; a review, 
J. Marketing Research 17,4 (1980) 407-422.
</blockquote><p>
Unfortunately, Cox found there is no universal "magic number" X for which we can say 
"X levels is the best choice."  Instead, the best number seems to depend on the application
and probably also on the statistical yardstick used to judge "best."
</p><p><small>
For example, for judging food, as we saw from the Peryam quote above, 11 may be the best number.
There have also been examples, e.g. patient-reported-outcomes in health care,
in which 5 levels appears best &ndash;
J.Khadka &amp; 4 others:
<a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3503574/">The importance 
of rating scales in measuring patient-reported outcomes</a>,
Health &amp; Quality of Life Outcomes 10 (July 2012) 80-;
and here an an example where 7 was superior to 5 levels:
Kraig Finstad:
Response Interpolation and Scale Sensitivity: Evidence Against 5-Point Scales,
J. Usability Studies 5,3 (May 2010) 104-110.
</small></p><p>
The psychometricians have tried many such yardsticks, although never exactly the one we want
(<a href="BayRegDum.html">Bayesian Regret</a>) for election purposes (nor is it common to
actually statistically measure "information content" using, e.g. entropy &ndash;
psychometricians unfortunately have historically been comparatively incompetent at statistics...
but we shall presume here that whatever analyses they were doing were good enough for this
comparatively trivial purpose).
Also, an extremely important question for us as election reformers is "which scale do 
voters <i>prefer</i>?" measuring how <i>enactible</i> different scale-based voting systems
would be if put to a referendum &ndash; but the psychometricians cared very little about that.
(We'll discuss that next section.)
Despite the absence of a single magic X, Cox was willing to provide <i>some</i> conclusions:
</p><ol type="A"><li>
Scales with two or three levels are "generally inadequate."
</li><li>
The marginal improvement from using more than nine levels is small.
The optimal number usually is 5, 6, 7, 8, or 9.
</li><li>
An odd number of levels is preferable (vs even) in situations where
respondent can legitimately adopt a neutral position.
But some pollsters have preferred even numbers of levels in an effort to <i>force</i> the
respondent to provide a non-neutral opinion, however slight.
<blockquote><small>
A noncommital paper on that was<br>
Ron Garland: 
<a href="assets/documents/MB_V2_N3_Garland.pdf">The Mid-Point on a Rating Scale: Is it Desirable?</a>
Marketing Bulletin 2 (1991) 66-70, Research Note 3.
</small></blockquote>
</li><li>
"Spatial" helps respondents, which Cox was vague about but I guess indicates
<i>graphical aid.</i>
</li></ol><p>
Later papers modified Cox's conclusions slightly:
</p><blockquote>
Gilbert A. Churchill Jr. &amp; J. Paul Peter: 
Research Design Effects on the Reliability of Rating Scales: A Meta Analysis, 
J. Marketing Research 21,4 (1984) 360-375.
</blockquote><p>
compiled evidence that "the more scale points used, the more 'reliable' the
scale."  Similar conclusions are supported by
</p><blockquote>
Duane F. Alwin:
Feeling thermometers versus 7-point scales &ndash; Which are better?
Sociological Methods and Research 25,3 (1997) 318-340.
</blockquote><p>
Mathematically speaking, certainly scales become better the more
levels they have (forever) in the sense that "roundoff error" is diminished.
However, those benefits increase more slowly the more levels we have, and
might be counteracted by other effects and disadvantages in the real world of human rather
than mathematically idealized respondents.
</p><blockquote>
Hershey H. &amp; Esther M. Friedman: A Comparison of Six Overall Evaluation Rating Scales,
Journal of International Marketing and Marketing Research 22,3 (1997) 129-138
</blockquote><p>
found that in some situations an 11-point scale produces more valid results than
a 3-, 5-, or 7-point scale. (See also the Peryam quote above.)
Their conclusion was that researchers should
consider using <b>anywhere from 5- to 11-point scales</b>, not 5 to 9 as Cox had
recommended.
</p>

<a name="best10"></a>
<h3> 10-level scale seems, overall, the best / most popular </h3>
<!--
Lily &amp; Yoram Neumann:
COMPARISON OF 6 LENGTHS OF RATING-SCALES - STUDENTS-ATTITUDES TOWARD
INSTRUCTION,
PSYCHOLOGICAL REPORTS 48,2 (1981) 399-404

Lily Neumann: Effects of scale length on means and correlation coefficients,
Quality and Quantity 17,5 (Sept 1983) 405-408
Latter shows 7 is better than 5 is better than 3 levels in terms of reducing
normalized std deviations.

And here is another ghastly paper which seems to think 9 & 10 levels best for its purposes:
http://www.cete.us/research/presentations/pdfs/2012_04_Shaftel%20et%20al.,%20Number%20of%20Response%20Categories,%204-9-12.pdf
Effects of the Number of Response Categories on Rating Scales
Roundtable presented at the annual conference of the American Educational Research Association , Vancouver, British Columbia
April 15, 2012
Julia Shaftel 
Brooke L. Nash 
Susan C. Gillmor

Michael S. Matell &amp; Jacob Jacoby:
Is there an optimal number of alternatives for Likert-scale items? 
Effects of testing time and scale properties,
J. Applied Psychology 56,6 (Dec 1972) 506-509. 
part I was Educational & Psychological Measurement 31 (1971) 657-674
[Answer was, apparently not]
-->
<blockquote>
Carolyn C. Preston &amp; Andrew M. Colman:
<a href="assets/documents/optinumb.pdf">
Optimal number of response categories in rating scales: reliability, validity,
discriminating power, and respondent preferences</a>,
Acta Psychologica 104,1 (2000) 1-15
</blockquote><p>
considered several statistical yardsticks for measuring the performance of rating scales,
including the comparatively unstudied (but very important for us) question of
<i>what the respondents wanted.</i>   They had 149 respondents fill out questionnaires
rating recently visited stores and restaurants on graphically-aided
scales with 2-to-11 levels (anchored with the words <i>very poor</i> 
on the left end and <i>very good</i> on the right) as well a 101-level scale where
the respondent was simply asked to write a number from 0 (very poor) to 100 (very good),
which was a different, not graphically aided, format.
They also were given another questionnaire 2 weeks later to assess test/retest reliability.
</p><p>
Brandon Wiley and W.D.Smith hereby decipher/summarize
Preston &amp; Colman's findings as follows,
where A<b>&gt;</b>B means "we have statistically significant evidence (&ge;95% confidence)
that A-level scale is superior to B-level scale in that respect"
while
A&sup;B means the superiority is not statistically significant.
(For example, 
for "validity" and "internal consistency" <i>no</i>
statistically significant differences were found at all.)
The <b>asterisked (*)</b> yardsticks "Easy,"
"Quick," and "Expressiveness" were <i>self-judged</i> by respondents.
</p>
<table bgcolor="yellow">
<tr><th>Statistical yardstick</th><th>Conclusions</th></tr>
<tr><td>Test-retest reliability</td><td>
9 = 8 &sup; 10 = 7 &sup; 11 = 6 &sup; 5 &sup; 101 &sup; 4 &sup; 2 &sup; 3
(significant: 6 <b>&gt;</b> 2 and 8 <b>&gt;</b> 101.)
</td></tr>

<tr><td>Internal consistency</td><td>
11 &sup; 101 = 10 = 9 = 8 = 7 &sup; 6 &sup; 5 = 4 &sup; 2 &sup; 3 (none signif.)
</td></tr>

<tr><td>Validity</td><td>
101 &sup; 9 &sup; 11 = 6 &sup; 10 = 8 = 7 = 5 &sup; 4 &sup; 2 &sup; 3 (none signif.)
</td></tr>

<tr><td>Item-whole correlations</td><td>
101 = 11 = 10 = 9 &sup; 8 = 7 &sup; 6 &sup; 5  <b>&gt;</b> 4 &sup; 3 <b>&gt;</b> 2
</td></tr>

<tr><td>Easy*</td><td>
5 <b>&gt;</b> 10 <b>&gt;</b> 7  <b>&gt;</b> 4  <b>&gt;</b> 8 &sup; 3 &sup; 6 &sup; 9  <b>&gt;</b>  2  <b>&gt;</b> 11 <b>&gt;</b> 101
</td></tr>

<tr><td>Quick*</td><td>
3 &sup; 2 <b>&gt;</b> 4 &sup; 5 <b>&gt;</b> 6 <b>&gt;</b> 7 &sup; 8 <b>&gt;</b> 10 <b>&gt;</b> 9 <b>&gt;</b> 11 <b>&gt;</b> 101
</td></tr>

<tr><td>Expressiveness*</td><td>
101 <b>&gt;</b> 10 <b>&gt;</b> 11 &sup; 9 <b>&gt;</b> 7 <b>&gt;</b> 8 <b>&gt;</b> 5 &sup; 6 <b>&gt;</b> 4 <b>&gt;</b> 3 <b>&gt;</b> 2
</td></tr>

<!--
<tr><td>Discriminating Power</td><td>
{2, 3, 4} <b>&lt;</b> {9, 101}</td></tr>
--
<tr><td>Overall Reliable</td><td>
{2, 3, 4} <b>&lt;</b> {5, 6, 11, 101} <b>&lt;</b> {7, 8, 9, 10}</td></tr>
--
<tr><td>Overall Discriminating</td><td>
{2, 3, 4} <b>&lt;</b> 5 <b>&lt;</b> {6, 7, 8, 9, 10, 11, 101}</td></tr>
--
<tr><td>Overall Preference</td><td>
10 &sup; 7 &sup; 9 <b>&gt;</b>  5 = 6 = 8 = 11 = 101 <b>&gt;</b> 2 = 3 = 4 
</td></tr>
-->

<tr><td>Intertertile Discriminating Power</td><td>
9 &sup; 101 &sup; 11 &sup; 10 &sup; 8 &sup; 6 &sup; 7 = 5 &sup; 2 &sup; 4 &sup; 3
(significant: 101 <b>&gt;</b> 3.)
</td></tr>
</table>
<p>
Preston &amp; Colman also considered correlations between ratings got from two different scales
for all possible scale-pairs. All scales correlated highly and significantly with each other,
but the best results arose for pairs of scales with &ge;6 levels each.
</p>
<p><small>
A.M. Colman, C.E. Norris, C.C. Preston:
<a href="assets/documents/comprati.pdf">Comparing rating scales of different lengths: Equivalence of scores from
5-point and 7-point scales</a>,
Psychological Reports 80,2 (April 1997) 355-362
<br>
showed the related result that converting between 5 and 7-point scales using linear interpolation,
works well.
</small></p>

<!--
8: ni=ei>te=se>el=sx>fv>hu>fo>tw>th
8: el>hu=te=ni=ei=se>sx>fv=fo>tw>th
8: hu>ni>el=sx>te=ei=se=fv>fo>tw>th
8: hu=el=te=ni>ei=se>sx>fv>fo>th>tw
9: fv>te>se>fo>ei>th>sx>ni>tw>el>hu
9: th>tw>fo>fv>sx>se>ei>te>ni>el>hu
9: hu>te>el>ni>se>ei>fv>sx>fo>th>tw
8: ni>hu>el>te>ei>sx>se=fv>tw>fo>th
With all 1: The Smith set is {el, hu, ni, te}.  The Schwartz set is {ni, te}.
There is no Condorcet winner.  ni wins with Borda and many COndorcet methods; also te wins
with many Condoircet methods.
With 2's for the three asterisked self-assessed preferences: te is Condorcet winner.
With 1,2 chnaged to 8,9: te is Condorcet winner. Also Borda.
http://www.cs.wustl.edu/~legrand/rbvote/calc.html
-->
<p>
If all 8 yardsticks in the above table are regarded as equally important, then
the "winner" would seem to be either 9 or 10 scale levels
in the sense that both 9 and 10 (uniquely)
are preferred by a <i>majority</i> of yardsticks versus each 
other rival (where "preferred" means either &sup; or <b>&gt;</b>).   
For example "10 levels" is preferred versus "5 levels" by 6 out of the 8
yardsticks, and versus "{4,3,2} levels" by 7 out of 8.
If (to break ties)
we regard the three self-judged (asterisked) categories
as slightly <i>more</i> important (accomplished by weighting them 1.1 rather than 1)
then <b>10 levels</b> becomes the unique-best
in the sense that 10 levels then is preferred by a (thus-weighted) 
majority of yardsticks versus <i>any</i> rival number
(where "preferred" means either &sup; or <b>&gt;</b>).
If we regard the statistically significant <b>&gt;</b> preferences
as more important than the &sup; ones, then the pre-eminence of <b>10 levels</b>
only becomes stronger.  However, if we regard the asterisked yardsticks as <i>tremendously</i>
more important than the others, then 5 levels would win, and other winners
would arise if other subsets of the yardsticks were awarded extreme importance.
</p>
<blockquote><small>
Thus in voting terminology, 10 is the "Condorcet winner" where the statistical yardsticks serve
as the (weighted) "voters" while the scale-cardinalities are the "candidates"
&ndash; also 10 is the Borda winner &ndash;
while with unweighted voters {9, 10} is the "Smith set" and 9 is the Borda winner.
(And with either unweighted or these-weighted voters, 9 is the IRV winner.)
Indeed, with our weighted voters there is a unique "Condorcet ordering" of the scale-sizes:
<center>
10 &gt; 9 &gt; 11 &gt; 101 &gt; 7 &gt; 8 &gt; 6 &gt; 5 &gt; 4 &gt; 3 &gt; 2
</center>
</small></blockquote>
<p>
Preston &amp; Colman did not actually give their results in the numberless
"&sup;" format above, instead tabulating
actual <i>numbers</i> measuring each scale-size's performance on each statistical yardstick.
If you <a href="assets/documents/optinumb.pdf">examine</a>
those numbers, then you'll probably agree that <b>10 levels</b> is
the winner.
</p><p>
Preston &amp; Colman's own conclusions were:
</p>
<blockquote>
"From the multiple indices of reliability, validity, discriminating power, and 
respondent preferences used in this study, a remarkably consistent set of conclusions emerges...
Scales with 2, 3, or 4 response categories yielded scores that were clearly and 
unambiguously the least reliable, valid, and discriminating.
The most reliable scores were those from scales with 7-10 response categories, 
the most valid and discriminating were from those with &ge;6 categories,
or &ndash; in the case of intertertile discriminating power &ndash; those with &ge;9.
Respondent preferences were that scales with 2, 3, or 4 categories once
again generally performed worst and those with 10, 9, or 7 best.
Taken together, [these] suggest that rating scales with 7, 9, or 10
response categories are generally to be preferred."
<br> &nbsp;&nbsp;&nbsp;&nbsp;
and (in their abstract)
<br> 
"Respondent preferences were highest for the 10-point scale."
</blockquote>

<p>
<b>Two related studies (each of more people) which approximately confirm Preston &amp; Colman:</b>
</p>
<blockquote>
E. Isaac Sparling &amp; Shilad Sen:
<a href="assets/documents/sen-recsys2011.pdf">Rating: How Difficult is It?</a>,
Proceedings of the fifth ACM conference on Recommender systems (RecSys 2011) 149-156
<br>
D. Cosley, S.K. Lam, I. Albert, J.A. Konstan, J. Riedl:
Is seeing believing?: how recommender system interfaces affect users' opinions. 
SIGCHI Proceedings (2003) 585-592, ACM New York, NY.
</blockquote><p>
Sparling &amp; Sen 
compared 4 rating systems for online use in rating (a) movies and (b) product reviews:
<ul>
<li>
"Unary": raters check a box to express approval.
</li><li>
"Thumbs up or down":  raters indicate either approval or disapproval by clicking
one of two symbols.
</li><li>
five-level rating scale (1-5 "stars")
</li><li>
"slider": effectively continuum-infinite rating scale.
</li></ul>
<p>
Of those, the clear winner for both purposes 
in terms of user expressed preferences, and also having fairly fast
response times, was <b>five star</b>.
</p><p>
Cosley et al compared three different systems:
<ul>
<li>
"binary": raters indicate either approval or disapproval by clicking
one of two symbols (same as "thumbs up/down")
</li><li>
{-3, -2, -1, +1, +2, +3} which has 6 levels since zero is intentionally <i>omitted</i>
</li><li>
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} regarded as an 0.5 to 5 "star" scale using 0.5-star increments.
</li></ul>
<p>
Among those, users clearly most-preferred 
<b>1-10</b>.
</p>
<p>
<b>An actual exit-poll score-voting 
<a href="WhatVotersWant.html">study</a> of real voters</b> in France 2012 concluded
voters preferred either of
the 3-level scales {0,1,2} or <nobr>{-1,0,+1}</nobr> over both
<a href="https://rangevoting.org/Approval/html">approval</a> voting (2-level scale {0,1})
and a 21-level scale <nobr>{0,1,2,3,..., 19,20}.</nobr>  
<small>
The authors speculated <nobr>{-2,-1,0,+1,+2}</nobr> would
be preferred even more, but did not directly study that.
</small>
</p><p>
All of these studies are compatible with
the conclusion 10-level scales are the most-wanted
by voters themselves.
</p>

<h3> Which numerical rating scales have pro pollsters used for rating politicians? </h3>
<p>
A different kind of "popularity" measure is this.
I searched the ORS's international historical poll <a href="http://www.orspub.com/">database</a>
of over 600,000 poll-questions. The following numerical score-ranges were used in questions
seeking ratings of politicians:
</p>
<table bgcolor="yellow" align="center">
<tr><th bgcolor="orange">Score range</th><td>&nbsp;</td><td align="center">
1-4</td><td align="center">
1-7</td><td>
0-100</td><td align="center">
<b>1-5</b></td><td>
1-100</td><td>
<b>1-10</b></td><td>
0-10</td><td>
ABCDF</td><td>
ABCDEF</td><td>
{-5 to -1}&cup;{+1 to +5}
</td>
</tr>
<tr><th bgcolor="orange">Popularity</th><td>&nbsp;</td><td align="center">
445</td><td align="center">
880</td><td align="center">
285</td><td align="center">
2700</td><td align="center">
64</td><td align="center">
1062</td><td align="center">
829</td><td align="center">
586</td><td align="center">
3</td><td align="center">
184</td><td align="center">
</td>
</tr>
</table>
<p>
but, apparently, no others.  [Here "popularity" is a very crude estimate based on
"hit count" in a separate too-simplistic automated search among <i>both</i> political and
non-political polls.]
<i>Initially</I> I had found <i>no</i> polls using negative numbers as ratings, 
but that unfortunately was
an artifact caused by the ORS's database search tool being confused by negative numbers,
as I realized after encountering these poll questions in the database
</p><table align="center" width="94%">
<tr><td>
KING, MARTIN LUTHER, JR.
<p>
Question: I'd like you to rate Dr. Martin Luther King on a scale. If you have a favorable opinion of him, name a number between +1 and +5 -- where a +5 is the highest position indicating you have a very favorable opinion of him. If you have an unfavorable opinion, name a number between -1 and -5 where -5 is the lowest position -- indicating you have a very unfavorable opinion of him. How far up or down the scale would you rate Martin Luther King Jr.?
</p><pre>
Source: USAToday/ Gallup
Date: 26 Aug. 2011
Universe: Country: United States
Method: telephone
Sample Size: 1319
</pre>
<td><td>
BUSH, GEORGE W., Opinion of
<p>
Question: Rate George W. Bush on a scale. If you have a favorable opinion of him, name a number between plus one and plus five - where a plus five is the highest position indicating you have a very favorable opinion of him. If you have an unfavorable opinion, name a number between minus one and minus five where minus five is the lowest position - indicating you have a very unfavorable opinion of him.
</p><pre>
Source: CNN/ USA Today/ Gallup Poll
Date: 25 Oct. 2004
Universe: Country: United States
Method: telephone
Sample Size: 1538
</pre>
</td></tr>
</table>
<p>
which the searches I had initially been using, would not find.
I did not investigate
(&le;3)-level numerical scales, but ultimately  
tried hopefully every reasonable possibility 0-to-X and 1-to-X with 4 or more
levels, also tried "negative X to Y" and "minus X to Y" and "between X and Y"
trying both numerals and spelled-out numbers ("five") with or without "plus" and "minus."
</p><p>
There also were
many <i>non</i>numerical verbal scales like 
"very positive, somewhat positive, neutral, somewhat negative, very negative"
or "favorable, unfavorable" most of which were "both-signed," so in that
sense negative numbers happened.  (Also other numerical scales besides those tabulated above
were used for rating things besides "overall quality of politicians.")
</p>
<pre>
 <b><u>phrase</u></b>    <b><u>popularity among pollsters</u></b>
unfavorable                  21988
fair                         19906
poor                         11354
disapprove                    8745
excellent                     8273
negative                      4317
neutral                       3859
very favorable                3637
somewhat unfavorable          2460
very poor                     2193
acceptable                    2054
somewhat negative             2017
very good                     1943
strongly favorable            1527
slightly                      1075
dislike                        972
very bad                       639
extremely good                  14
superb                          0
</pre><p>
The key <b>lessons</b> seem to be that pro pollsters 
<p><ul><li>
most prefer 
5- and 10-level scales for politician-rating purposes (if using a numerical scale), 
</li><li>
prefer numerical over letter-grade, 
</li><li>
prefer to avoid negative numbers, 
</li><li>
with 2-to-4 levels they prefer verbal over numerical scales, 
</li><li>
but with &ge;7 levels they prefer numerical over verbal scales,
</li><li>
with 5 or 6 levels, verbal or numerical seem about equally popular,
</li><li>
if using a verbal scale, prefer "balanced" over "unbalanced."
</li></ul>


<h3> What about continuum-infinite "graphical slider" scales? </h3>

<blockquote>
Max Freyd: The graphic rating scale,
Journal of Educational Psychology 14,2 (Feb, 1923) 83-102.
</blockquote>
<p>
pointed out advantages of a <i>continuum infinite</i> rating scale where the respondent 
makes a mark on a line segment to indicate her rating.
("Concrete, simple, universal, easy to administer and score.")
The only disadvantage Freyd could see was it is more annoying for the pollster
to process graphical polls (have to measure lengths).
For voting purposes, though, that could be devastating (can you imagine a lawsuit 
about the precision of length measurements?).
Despite Freyd's arguments, over the next 90 years his idea did not gain wide use.
With the advent of computer graphics and the internet, <b>"graphical sliders"</b>
became usable, avoiding the only criticisms (albeit for voting purposes, we could still
imagine nasty possible lawsuits and might need to try to avoid them by precise legal
specifications about every pixel of the "graphical slider"... a nightmare... and
even then: what about blind voters? Or those with reduced visual acuity?  And
what if somebody accuses a machine of slightly biasing things by moving 1 pixel left?).
</p><p>
At least two studies were made trying to assess the usefulness of this new tool.
The surprising conclusion from those studies is that <i>sliders do not work as
well as "radio buttons"</i>:
</p>
<table  align="center" bgcolor="#FFCC00">
<tr><td>
<img src="assets/images/SliderBadGood.png"/>
</td><td>
<form><table>
<tr>
<td colspan="12" align="center"><big><big>RADIO BUTTONS</big></big></td>
</tr><tr><td>&nbsp;</td></tr><tr>
<td></td>
<td align="center">0
</td><td align="center">1
</td><td align="center">2
</td><td align="center">3
</td><td align="center">4
</td><td align="center">5
</td><td align="center">6
</td><td align="center">7
</td><td align="center">8
</td><td align="center">9
</td><td></td>
</tr>
<td>Bad</td>
<td><input type="radio" name="radbut">
</td><td><input type="radio" name="radbut">
</td><td><input type="radio" name="radbut">
</td><td><input type="radio" name="radbut">
</td><td><input type="radio" name="radbut">
</td><td><input type="radio" name="radbut">
</td><td><input type="radio" name="radbut">
</td><td><input type="radio" name="radbut" checked="checked">
</td><td><input type="radio" name="radbut">
</td><td><input type="radio" name="radbut">
</td>
<td>Good</td>
</tr>
</table>
</form>
</td></tr>
</table>
<p>
The studies were:
</p><blockquote>
Mick P. Couper, Roger Tourangeau, Frederick G. Conrad, Eleanor Singer:
Evaluating the Effectiveness of Visual Analog Scales: A Web Experiment,
Social Science Computer Review 24,2 (2006) 227-245
<!-- http://www.odum.unc.edu/content/pdf/couper%20visual%20analog%20scales.pdf -->
<br>
Colleen Cook, Fred Heath, Russel L. Thompson and Bruce Thompson:
Score Reliability in Web- or Internet-Based Surveys: 
Unnumbered Graphic Rating Scales versus Likert-Type Scales,
Educational and Psychological Measurement 61,4 (2001) 697-706
<!--
http://classdat.appstate.edu/COB/MGT/VillanPD/Performance%20Management%202013/RESERVE/6.%20Rating%20Format%20Research%20&%20Advances/Cook%20et%20al%202001-Web-based%20response%20scales.pdf
-->
<br>
Randall K. Thomas:
<a href="assets/documents/RThomasVAS.pdf">A Comparison of Visual Analog and Graphic Rating Scales in
Web-based Surveys</a>, FedCASIS 2011 workshop, Washington DC.
</blockquote><p>
The first study found significant effects (&gt;99.9% confidence)
of the input type on the overall completion rate (of internet surveys):
completion rates of 91.8% for the slider, 
<b>95.8%</b> for the radio buttons, and 95.6% for the 
numeric input versions.
Also (which is probably related), all three studies found radio buttons continually
produced the <i>fastest</i> answer-time clockings, with sliders the <i>slowest</i>.
In Thomas's study the slider/radio-button time-ratio was 1.46;
in the study by Cook et al, it was only 1.11.
</p><p>
However, neither of the first two studies were
able to detect significant differences in answer quality
got by the various input methods.  
The third study (which was far larger!) by Thomas, while unable to see a difference in 
"validity," did succeed in seeing this:
the <i>lowest standard deviations in ratings</i> were obtained with a graphic
end-anchored scale (no intermediate anchors), namely the graphic scale gave 
&sigma;=0.19 
per respondent,
which was superior to end-anchored radio buttons with
&sigma;=0.22
(both normalized to a real number scale ranging between 0 and 1 &ndash; I thank Thomas for
clarifying this point).
<i>This</i> difference appears to be statistically significant, albeit too
small to be detected by polls with only 1000 respondents.
</p><p>
From Thomas's standard-deviation study we learn the important lesson that 
<b>it is best to use end-anchored scales</b> and <i>not</i> intermediate anchors.
That is, using a line of radio buttons, or a line segment for slider input,
only label the <i>outer two buttons</i>, or line-segment <i>ends</i>,
with descriptive words.  
Do not label interior points with descriptions.
(Even merely placing an unlabeled "grid" along the line segment seems incommodious.)
</p><table bgcolor="PapayaWhip" align="center"><tr><td>
<pre>
  <font color="red">DO THIS</font>:             bad |---------------| good       bad o  o  o  o  o  o  o  o good      

  <font color="red">DO <b>NOT</b> DO THESE</font>:
                                 o   o   o   o   o   o   o 
      |----+----+----|         very     bad     good    very      bad |---+---+---+---| good
    very  bad  good very        bad                     good
    bad             good
</pre></td></tr></table>
<p>
If you do, then Thomas's study shows you will 
</p>
<ol type="a">
<li>
Slow response times,
</li><li>
Increase standard deviations in responses,
</li><li>
Complicate the ballot,
</li>
</ol>
<p>
all of which are undesirable.
</p><p>
<small>
Incidentally, it also is possible to consider scales with <i>no</i> anchors,
also called <b>"self-anchoring"</b> scales, in which the voter herself mentally imagines
whatever two end-anchoring words she likes.   
Frederik Van Acker &amp; Peter Theuns:
<a href="assets/documents/JSM2007-000352.pdf">Assessing the Applicability of Self-Anchoring Scales in Web Surveys</a>,
Proceedings Survey Research Methods Section, American Statistical Association (2007)
studied them and concluded they were a bad idea.
</small>
</p><p>
In the light of the first two studies alone, there seems little or no reason to
use graphic scales &ndash; if there is no measurable quality improvement, why
employ the slower (and in the first study less-completed) slider method?
(Plus, we remind the reader of an earlier mention of
Sparling &amp; Sen's study finding <i>users</i> preferred a 5-level discrete scale over a slider.)
But the third study <i>did</i> find a small, but detectable, quality improvement,
partly just due to it being a larger study, and partly (perhaps) due to the fact it tried
(among other things) end-anchored-only scales, which as we said are best.
Even if so, though, for <i>voting</i> purposes the possible
slight statistical advantages not seem worth the possible legal quagmires.
</p>

<!--
It has been 
suggested that for "rating cinematic films" 11 

LAMscale.jpg
<td>"Labeled effective magnitude scale" by Schutz &amp; Cardello 2001</td>
-->

<h3> Nonnegative versus both-signed scales </h3>

<blockquote>
Norbert Schwarz, Barbel Knauper, Hans J. Hippler, Elisabeth Noelle-Neumann,  Leslie Clark:
Numeric Values May Change the Meaning of Scale Labels, 
Public Opinion Quarterly 55,4 (1991) 570-582.
</blockquote>
<p>
The responses of German adults to the question
"How successful would you say you have been in life?" 
were influenced by the numeric values provided to give meaning to the scale labels. 
When the scale ranged from 0 ("not at all successful") to 10 ("extremely successful"), 
<b>34%</b> selected values between 0 and 5. 
But when the scale went from -5 ("not at all successful") to +5 ("extremely successful"), 
only <b>13%</b> selected values -5 to 0. 
</p>
<blockquote>
Norbert Schwarz &amp; Hans J.Hippler:
<a href="assets/documents/SchwarzHipplerPolit.pdf">The 
Numeric Values of Rating Scales: A Comparison of their Impact in Mail Surveys and 
Telephone Interviews</a>,
Int'l J. Public Opinion Research 7,1 (1995) 72-74.
<br>
G.Haddock &amp; R.Carrick:
How to make a politician more likeable and effective: Framing
political judgments through the numeric values of a rating scale,
Social Cognition 17,3 (1999) 298-311.
</blockquote>
Schwarz &amp; Hippler selected
412 Germans randomly from telephone directories
and asked them to
rate 6 <b>politicians</b> along 11-point scales with different numeric values, 
ranging from "don't think very highly" (0 or -5) to 
"think very highly of this politician" 
(10 or +5).
Half did so orally by telephone and the other half (randomly chosen) in 
writing by mail.
As predicted by the preceding study, <i>all</i>
politicians received more favorable ratings along the -5 to +5 
than along the 0 to 10 scale. 
Specifically (renormalizing everything to lie on the 0-10 scale
for comparison purposes) the -5 to 5 scale yielded mean=5.6 while the 0-10 
scale yielded
mean rating 5.0.
In all, 29.3% reported a mean approval rating below the midpoint 
along the -5/+5 scales, 
whereas 40.2% did so along the 0-10 scale.
Moreover, this effect happened with <i>both</i> data 
collection methods, demonstrating that the impact of numeric values 
does not depend on their 
visual presentation. The telephone question wording was:
</p><blockquote>
Please imagine a thermometer that runs from minus five to plus five, with a zero in between 
(alternate wording: that runs from zero to ten). Please use 
this thermometer to tell us how you feel about 
some politicians. Plus five (ten) means that you think very highly of them, and 
minus five (0) means that you don't think very highly of them. 
How do you feel about (name)?
</blockquote>
<p>
The same result was replicated by Haddock &amp; Carrick 
in surveys rating Prime Minister Tony Blair conducted the day before the 1997 British election.
</p><p>
I have also replicated this result, to some degree:
<ul><li>
I collected 5 score-voting-style polls using the score range {1,2,3,...,10}
conducted by Elon University in N.Carolina (sample size&asymp;411 each)
asking for ratings of these politicians:
<center>
John McCain 19 sep 2008,
Barack Obama 19 sep 2008,
Sarah Palin 19 sep 2008,<br>
Elizabeth Dole 3 oct 2008,
Joseph Biden 18 sep 2008.
</center>
Summed score-usage results: <b>1-5: 259.1%; &nbsp; 6-10: 198.2%.</b>
</li><li>
I collected 5 score-voting-style polls using the score range {-5,-4,-3,-2,-1, +1,+2,+3,+4,+5}
(note zero is <i>omitted</i>)
conducted by Gallup in United States (sample size&asymp;1538 each)
asking for ratings of these politicians:
<center>
G.W.Bush 25 oct 2004,
John Kerry 25 oct 2004,<br>
Edward Kennedy 16 dec 1991,
George Bush 23 oct 1991,
Dan Quayle 20 aug 1991
</center>
Summed score-usage results: <b>(-5 to -1): 191%; &nbsp; (+1 to +5): 287%.</b>
</li><li>
I found a USA-wide score-voting-style poll (6 sep 1987) rating 13 politicians on a
{-5,-4,...+5} scale (here zero is <i>included</i>) done by Gallup for the Los Angeles Times.
(Sample size 1607.)
The results apparently were (with 0-scores discarded; also for both this and 
the preceding polls we've discarded "don't know"s)
<pre>
<b>     Politician        -5 to -1  +1 to +5</b>
   Michael Dukakis        35%      29%   
       Albert Gore        25       26  
        Paul Simon        22       17    
      Joseph Biden        20       28 
     Jesse Jackson        15       84    
  Richard Gephardt        13       24  
     Bruce Babbitt        10       17    
     George Bush          29       87  
        Bob Dole          28       61  
       Jack Kemp          21       37  
    Pete du Pont          19       16   
  Alexander Haig          14       67 
   Pat Robertson           9       55   
<b>       sum               260      548</b> 
</pre>
(More, albeit still incomplete, <a href="assets/documents/r110.txt">details</a> about that data.)
</li></ul>
This once again agrees with the theory that <b>using an all-nonnegative scale causes
better scale-use uniformity</b>, while scales including negative numbers cause 
under-use of the negative part of the scale.   It should be noted that the scale-use-uniformity 
is desirable for the most <i>prominent</i> politicians (i.e those likely to win); 
nonuniformity is ok for losers since lower accuracy on them will damage election results less.
All of the above studies were for prominent politicians only.
</p><p>
<b>Conclusion:</b>
Both the above studies, plus my own replication from a much larger 
amount of published professional pollster data, all suggest
it is better to use a unipolar 0 to 10 than a both-signed -5 to +5 scale
for 
<a href="RangeVoting.html">score voting</a>, in the sense that
the former should yield more-uniformly distributed scores.
This seems true regardless of whether zero is omitted or included.
The problem is that <b>with <nobr>-5</nobr> to <nobr>+5</nobr>, 
the lower half of the scale is largely unused and wasted.</b>
</p><p>
On the other hand in an exit-poll 
<a href="France2012.html">study</a> of French 2012 presidential election voters,
much more uniform usage of the scale was found for 
<nobr>{-1, 0, +1}</nobr> than with <nobr>{0, 1, 2},</nobr>
suggesting that in this case, the <i>both-signed</i> scale is superior.  
That is probably due to our next effect...:
</p>

<h3> An inordinate fondness for zero </h3>

<blockquote>
He has an inordinate fondness for beetles.
<br>&nbsp;&nbsp;&nbsp;
&ndash; J.B.S.Haldane (Naturalist) upon being asked what his study of the 
animal world could tell us about God.
</blockquote><p>
It is a strange thing, but in every score voting poll
(or using other rating-based voting systems, e.g. 
&quot;<a href="MedianVrange.html">Majority Judgment</a>&quot;) that has
closely simulated real election conditions (that I am aware of as of 2013), the most popular
score has been <i>zero</i>.
</p><p>
Illogically,
this happens whether or not the minimum-allowed score is zero &ndash; i.e. it also happens
for both-signed scales such as <nobr>{-1, 0, +1}.</nobr>
With the Majority-Judgment <i>verbal</i> scale
recommended by Balinski &amp; Laraki, the minimum score "a Rejeter" plays zero's role.
</p><p>
To discourage this, some pollsters have resorted to the trick of making the allowable scores be
{-5, -4, -3, -2, -1, +1, +2, +3, +4, +5} with zero <i>omitted</i>.
</p><p>
But there <i>may</i> be a way out of this 
black hole.   The University of Iowa during 1998-2000 conducted a
large number of score-voting <a href="assets/documents/UIowaPolls.txt">polls</a>, 
using an 0-100 scale,
in various mid-US states concerning various national and 
that-state politicians.  Their instructions said (my italics)
</p>
<blockquote>
Let's talk about your feelings toward some prominent people.
I'll read the name of a person and ask you to rate that person on a
thermometer that runs from 0 to 100 degrees. <i>A rating of 50 degrees is neutral.</i> 
Ratings between 51 and 100 degrees mean that you feel warm
toward that person, and ratings between 0 and 49 mean that you feel
cool toward that person. You may use any number from 0 to 100 to tell
me how favorable or unfavorable your feelings are for each person. If
you don't recognize a particular person just tell me and we will go on
to the next name. Pat Buchanan.
</blockquote><p>
In the U.Iowa poll series, the most popular score was 50, not zero.
(This contrasts with my own exit-poll simulated election <a href="PsEl04.html">study</a>
using 0-100 scale, in which 0 was most popular and nothing was mentioned about 50 in the 
instructions.)
</p><p>
The U.Iowa polls were mere polls, not conducted under conditions 
closely-simulating a real election.   Also, they concerned well-known politicians only,
as opposed to the studies of real elections which included lesser-known candidates.
But for what it is worth, this suggests that <b>perhaps
the inordinate-fondness can be shifted from 0 to 50 just by the power of suggestion.</b>
This effect also shows the importance of making the ballot instructions <i>the same</i>
everywhere in the country.
</p>


<h3> Rating is superior to Ranking </h3>

<p>
Which is the better way to elicit information about how people feel about
various things &ndash; rating them on some fixed numerical "quality scale," or
ranking them into order of increasing quality?  As you can tell from the quote
at the top of this page, there is a long-standing
consensus that <i>rating</i> is usually better. 
That began to develop with
</p><blockquote>
E.S.Conklin &amp; J.W.Sutherland:
A Comparison of the Scale of Values Method with the Order-of-Merit Method,
J. Experimental Psychology 6,1 (Feb. 1923) 44-57.
<br>
&nbsp;&nbsp;&nbsp; Abstract:
In one series of experiments forty jokes were arranged by ten subjects
by the order-of-merit method; in another series the same jokes were arranged by
ten other subjects by the scale-of-values method. The correlation between the
final two orders was 0.55. A closer examination of the statistical data
indicated that the scale-of-values method was a better index...
</blockquote><op>
The most obvious advantage of ratings is that they elicit <i>more information</i>.
A bit less obviously, people rate things <i>faster</i> than they rank them.
Thus
</p><blockquote>
J.Michael Munson &amp; Shelby H. McIntyre:
Developing practical procedures for the measurement of personal values
in cross cultural marketing, J. Marketing Research 16,1 (1979) 48-52
</blockquote><p>
estimated (p.49) that in their experiments, ranking took <i>3 times longer.</i>
</p><p>
The possible counterarguments 
in favor of rankings would be that they are somehow more
"meaningful" and for that or some other reason (perhaps including 
the greater time consumption) yield better-quality assessments.
This was <i>especially</i>
suspected for such vague purposes as "rating personal values"
such as "sense of accomplishment" versus "freedom" versus "wisdom"
(and in fact was contended by the above paper).
Well, the following papers by Feather and Moore
compared ratings versus rankings in <i>exactly</i> that 
most-favorable-to-rankings scenario &ndash; the "Rokeach value survey":
</p><blockquote>
Michael Moore:
Rating versus ranking in Rokeach value survey &ndash; Israeli comparison,
Europ. J. Social Psychology 5,3 (1975) 405-408.
<br>
N.T.Feather: The measurement of values: effects of different assessment procedures,
Australian J. Psychology 25,3 (1973) 221-231
</blockquote><p>
Moore's conclusion: "The extremely high degree of correspondence found between 
ranking and rating in this study, as
well as in Feather 1973, does not leave any 
doubt about the preferability of the rating method for group description purposes."
</p><p>
Case closed.
</p><p>
But for anybody still denying this, we also mention the following 
additional papers:
</p><blockquote>
M.F.Johnson, J.F.Sallis, M.F.Hovell:
Comparison of rated and ranked health and lifestyle values,
Amer.J.Health Behavior 23,5 (Sep-Oct. 1999) 356-367
("Results from the present study suggest that a rating
format for health and lifestyle value items should be used.")
<br>
Markus Klein &amp; Kai Arzheimer:
Ranking and rating procedures for the measurement of values, analyzed
with an example of the Inglehart-index. Empirical results of a
methodological experiment,
Kolner Zeitschrift f&uuml;r Soziologie und Socialpsychologie 51,3 (Sep.1999) 550-564
[see also <i>Einmal mehr: Ranking oder Rating?</i> 52,3 (Sept. 2000) 541-552 &amp; 553-563].
<br>
&nbsp;&nbsp;&nbsp; Abstract:
In empirical social research there is a long debate about
the question, whether ranking procedures are indeed more appropriate
for the measurement of value orientations than rating scales. To gain
empirical clarity, we incorporated a methodological experiment into a
two-wave mail panel survey. In the first wave we asked respondents to
rank and rate the four items of the well known Inglehart-Index. In the
second wave we split the sample and while asking the same question, we
changed the order of the items in the questionnaire for half of the
respondents. By means of this experimental design we can demonstrate
that there exists a correlation between the order of the items in the
questionnaire and the rank-order that respondents assign to these
items. This relationship is even stronger for respondents which gave
the same rating to all items in the first panel wave. Thus, ranking
procedures seem to be affected by response sets as well as rating
procedures. <i>In the light of these findings, there remains no basis for
the assumption, that rankings are superior to ratings in
methodological terms.</i>
<br>
G.R.Maio, N.J.Roese, C.Seligman, A.Katz:
Rankings, ratings, and the measurement of values: Evidence for the
superior validity of ratings,
Basic and Applied Social Psychology 18,2 (June 1996) 171-181.
("[Our statistical] results indicated that ratings tended to
evidence greater validity than rankings within moderate- and
low-differentiating participants. In addition, the validity of ratings
was greater than rankings overall.")
</blockquote><p>
It seems no longer possible to dispute that ratings are superior, since they are so
even in settings like those above which initially had been
conjectured highly favorable to rankings.
</p>

<h3> Differences between real elections versus polls </h3>

<p>
Certain issues loom much larger in real elections than in mere polls.
</p><p>
In polls, the pollster need not worry much about disabled people (e.g. partially or totally
blind or deaf).  Nor is he likely to be sued because of some detail of his practices.
But in elections, these are serious concerns.
</p><p>
For example, 
different <a href="MeasTheory.html#quantadj">adjectives</a> mean different things
to different people (e.g. different socio-ethnic groups), which could bias elections.
It might be argued that using adjectives <i>at all</i> disadvantages poorer speakers of English
(relatively speaking).  Thus there might be a good legal/fairness
argument for using <i>digits</i>, 
not words, to describe scores used for voting, even if the words were somewhat
superior overall reckoned by some statistical measure &ndash; the digits are more concise
and have an unambiguous and simple meaning
(mathematically defined by the rules of the voting system),
comprehensible to all cultures and all
ethno-lingual groups, maximally easily understood by sensory-disabled people,
and not susceptible to any possible accusation of unfairness.
</p><p>
Slight error and confusion rates, are of little concern to pollsters but of 
tremendous concern in elections and in legal battles about elections.
</p><p>
Also, on ballots, conciseness, speed, and simplicity
are desirable.   Printing 300 million ballots is
not a trivial expense, and time delays have been used to manipulate elections by
creating long waiting lines at
polling places, e.g. in Ohio 2004.
</p><p>
Finally, let us discuss <b>voter behavior</b> differences in polls versus real elections.
Voters may be more "strategic" in real elections than in polls.   I have seen
the following indication of that: it <a href="PollAccurRecord.html">appears</a>
(but only with about 90% confidence based on the data I have seen so far) 
that in the USA 1980-2012, people overstated their support of
third-party candidates to pollsters, relative to when they actually voted
(both these polls and official elections using 
plain-<a href="Plurality.html">plurality</a> voting).
</p><p>
In the U.Iowa telephone score-style 
<a href="assets/documents/UIowaPolls.txt">polls</a> using 0-100 scale, respondents 
stayed entirely away from scores such as "73." They almost entirely
used multiples of 10 or 25, less often employing other multiples of 5, 
with the <i>only</i> used non-multiples of 5 being
1, 49, and 99:
</p>
<img src="assets/images/UIowa28distns.png"/>
<p>
In contrast, in my own <a href="PsEl04.html">smaller</a>
score (0-100) election exit poll (in person using 
written ballots and much more closely simulating areal election)
use of non-multiples of 5 was (while still rare) much more frequent, including uses
of 1, 2, 3, 4, 21, 64, 72, 73, and 98.
<b>The reason for this discrepancy</b>
probably is: the Iowa respondents on telephone were unable to know
the full set of "candidates" in advance
since they were only told them one at a time, and hence were unable to
consider fine preferences between them.  In my election-simulating exit poll,
they had the full "ballot" in front of them and wrote on it.
</p><p>
  Either way, this effect suggests that an
<b>0-100 range is overkill</b> because 95% of the voters choose to restrict themselves to the
21-level subset {0,5,10,...95,100}.  
It perhaps argues for "fine at the ends" scales such as {0,1,25,50,75,99,100}.
</p>

<a name="conclusions"></a>
<h3> <u>Conclusions</u> </h3>

<p>
The evidence surveyed here currently
suggests that the "best" scale for human voters
should have <b>10 levels</b> and consist entirely of <b>nonnegative numbers</b>
ordered <b>increasing from left to right</b>, and <b>equispaced</b>,
with the two <b>endpoints</b> <i>only</i>
of the scale "anchored" with descriptive words.
Ballot instructions should have <b>exactly the same standard wording</b> mandated everywhere in
the country (and be simply and concisely worded).
</p><p>
Here "best" is reckoned by a variety of statistical yardsticks applied to data from real humans,
plus the <a href="#best10">preferences</a> of those voters themselves.
And almost certainly the best number of scale-levels lies in {5, 6, 7, 8, 9, 10, 11}
and we know of at least <a href="Why99.html#aubrygraphics">one</a> important election in
which at least 7 scale levels were genuinely needed by the voters to describe the leading 
candidate.
</p><p>
<small>
This <i>contradicts</i> my initial conjectures that the best scale would be continuum infinite 
such as the real-number interval [0,1].  That is mathematically the most elegant
and yields the best results in Bayesian-regret-based computer simulations of
elections.
Failing that, I had suspected that large-cardinality scales, such as "0 to 99" or
"0 to 999," would be best.   
</small>
</p>

<a name="openquest"></a>
<h3> Open experimental questions </h3>
<ol>
<li>
Is the "inordinate fondness for zero" only present in real elections (or polls 
designed to closely simulate "real" voting conditions) and nonexistent in "mere polls",
and how modifiable is it by use of different ballot instructions?  What is the optimum wording 
for the ballot instructions?
</li><li>
Is an 0-9 scale better or worse than 1-10?
</li><li>
Are score voters
more strategic, e.g. use the extreme ends of the scale more, in real elections
versus in mere polls?
</li>
</ol>


<br>
<p><a href="WhatVotersWant.html">What Voters Want</a></p>
<p><a href="RangeVoting.html">Return to main page</a></p>

<!-- Start of StatCounter Code -->
<script type="text/javascript" language="javascript">
var sc_project=1613646; 
var sc_invisible=1; 
var sc_partition=15; 
var sc_security="a35ff8fb"; 
</script>

<script type="text/javascript" language="javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><a href="http://www.statcounter.com/" target="_blank"><img  src="http://c16.statcounter.com/counter.php?sc_project=1613646&amp;java=0&amp;security=a35ff8fb&amp;invisible=1" alt="php hit counter" border="0"></a> </noscript>
<!-- End of StatCounter Code to be inserted immediately before the /body command near end of your page -->
</body>
</html>

